{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c8dd97-7e79-4c57-863e-5f1882d73eee",
   "metadata": {},
   "source": [
    "\n",
    "## This code collects track data from the Deezer API, filters it by release year (2000–2025), and saves it to a CSV file. It handles resuming from the last processed track and ensures no duplicates by checking existing track IDs.\n",
    "\n",
    "- **Setup:** constants like the target number of tracks and file paths are defined\n",
    "- **Load Existing Data:** If a dataset exists, it is loaded if not a new df is made\n",
    "- **Track Data Fetching:** The code fetches track details from the Deezer API and filters by year\n",
    "- **Data Saving:** Every 1000 valid tracks, the data is saved to the CSV file, it also resumes from the last processed track using a saved ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833cb9d-52dd-4eab-8580-4c45395e8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))  # .py\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()  \n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, \"../../Data/Raw\"))\n",
    "os.makedirs(BASE_DIR, exist_ok=True)  \n",
    "\n",
    "# Constants\n",
    "START_ID = 1_305_200\n",
    "VALID_TRACKS_TARGET = 600_000\n",
    "TRACKS_TO_SAVE_EVERY = 1000\n",
    "SLEEP_TIME = 0.20\n",
    "LAST_PROCESSED_ID_FILE = os.path.join(BASE_DIR, \"last_processed_id.txt\")\n",
    "csv_file = os.path.join(BASE_DIR, \"deezer_tracks_super_dataset1.csv\")\n",
    "\n",
    "YEAR_MIN = 2000\n",
    "YEAR_MAX = 2025\n",
    "\n",
    "#Load Existing data\n",
    "if os.path.exists(csv_file):\n",
    "    existing_df = pd.read_csv(csv_file, low_memory=False)\n",
    "    collected_so_far = len(existing_df)\n",
    "    print(f\"Loaded existing dataset with {collected_so_far} tracks.\")\n",
    "    \n",
    "    if \"id\" in existing_df.columns:\n",
    "        existing_ids = set(existing_df[\"id\"])\n",
    "    else:\n",
    "        print(\"Warning: 'id' column not found. Deduplication will use title/artist/album.\")\n",
    "        existing_ids = set()\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    collected_so_far = 0\n",
    "    existing_ids = set()\n",
    "\n",
    "#Fetch Track from Deezer \n",
    "def fetch_track(track_id):\n",
    "    url = f\"https://api.deezer.com/track/{track_id}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"error\" in data:\n",
    "                return None\n",
    "            return {\n",
    "                \"id\": data.get(\"id\"),\n",
    "                \"title\": data.get(\"title\"),\n",
    "                \"artist\": data[\"artist\"][\"name\"] if \"artist\" in data else None,\n",
    "                \"album\": data[\"album\"][\"title\"] if \"album\" in data else None,\n",
    "                \"duration\": data.get(\"duration\"),\n",
    "                \"rank\": data.get(\"rank\"),\n",
    "                \"explicit_lyrics\": data.get(\"explicit_lyrics\"),\n",
    "                \"release_date\": data.get(\"release_date\"),\n",
    "                \"isrc\": data.get(\"isrc\"),\n",
    "                \"deezer_link\": f\"https://www.deezer.com/track/{data['id']}\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching track {track_id}: {e}\")\n",
    "    return None\n",
    "\n",
    "#Resume from Last Track\n",
    "if os.path.exists(LAST_PROCESSED_ID_FILE):\n",
    "    with open(LAST_PROCESSED_ID_FILE, 'r') as file:\n",
    "        track_id = int(file.read().strip())\n",
    "        print(f\"Resuming from track ID: {track_id}\")\n",
    "else:\n",
    "    track_id = START_ID\n",
    "\n",
    "valid_tracks = []\n",
    "\n",
    "# Loop \n",
    "while collected_so_far + len(valid_tracks) < VALID_TRACKS_TARGET:\n",
    "    if track_id in existing_ids:\n",
    "        track_id += 1\n",
    "        continue\n",
    "\n",
    "    track_data = fetch_track(track_id)\n",
    "    if track_data:\n",
    "        release_year = None\n",
    "        if track_data.get(\"release_date\"):\n",
    "            try:\n",
    "                release_year = int(track_data[\"release_date\"][:4])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if release_year and YEAR_MIN <= release_year <= YEAR_MAX:\n",
    "            valid_tracks.append(track_data)\n",
    "        else:\n",
    "            print(f\"Skipped track {track_id} (year {release_year}) — not in {YEAR_MIN}–{YEAR_MAX}\")\n",
    "    else:\n",
    "        print(f\"Invalid or missing track ID: {track_id}\")\n",
    "\n",
    "    if len(valid_tracks) % TRACKS_TO_SAVE_EVERY == 0 and valid_tracks:\n",
    "        new_df = pd.DataFrame(valid_tracks)\n",
    "        if not existing_df.empty:\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "            combined_df.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "        else:\n",
    "            combined_df = new_df.drop_duplicates(subset=[\"id\"])\n",
    "        combined_df.to_csv(csv_file, index=False)\n",
    "        print(f\"Saved {len(combined_df)} total unique track.\")\n",
    "        existing_df = combined_df.copy()\n",
    "        existing_ids = set(existing_df[\"id\"])\n",
    "        valid_tracks = []\n",
    "\n",
    "        # Save current ID\n",
    "        with open(LAST_PROCESSED_ID_FILE, 'w') as file:\n",
    "            file.write(str(track_id))\n",
    "\n",
    "    track_id += 1\n",
    "    time.sleep(SLEEP_TIME)\n",
    "\n",
    "#  Save\n",
    "if valid_tracks:\n",
    "    new_df = pd.DataFrame(valid_tracks)\n",
    "    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    combined_df.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "    combined_df.to_csv(csv_file, index=False)\n",
    "    print(f\"Final save: {len(combined_df)} total unique tracks.\")\n",
    "else:\n",
    "    print(\"No new tracks collected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13932c-3f42-4879-97a6-7de7cbf97004",
   "metadata": {},
   "source": [
    "## This code cleans and saves Deezer track data while handling missing data and API limits. \n",
    "- **Data Cleaning**: Handles missing metadata by re-fetching and skips invalid tracks\n",
    "- **Concurrency**: Uses parallel fetching for faster processing.\n",
    "- **Rate Limiting**: Implements exponential backoff on quota hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants and directories\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../Data/Raw\"))  \n",
    "PROCESSED_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../Data/Processed\"))  \n",
    "SCRAPER_DIR = os.getcwd()\n",
    "\n",
    "START_ID = 1_305_200\n",
    "VALID_TRACKS_TARGET = 600_000\n",
    "TRACKS_TO_SAVE_EVERY = 1000\n",
    "SLEEP_TIME = 0.15\n",
    "\n",
    "LAST_PROCESSED_ID_FILE = os.path.join(BASE_DIR, \"last_processed_id.txt\")\n",
    "CLEANING_DONE_FILE = os.path.join(BASE_DIR, \"cleaning_done.txt\")\n",
    "CSV_FILE = os.path.join(BASE_DIR, \"deezer_tracks_super_dataset1.csv\")\n",
    "\n",
    "YEAR_MIN = 2000\n",
    "YEAR_MAX = 2025\n",
    "\n",
    "#  Skip cleaning if already done \n",
    "SKIP_CLEANING = os.path.exists(CLEANING_DONE_FILE)\n",
    "\n",
    "# Load existing dataset \n",
    "if os.path.exists(CSV_FILE):\n",
    "    existing_df = pd.read_csv(CSV_FILE, low_memory=False)\n",
    "    collected_so_far = len(existing_df)\n",
    "    print(f\"Found existing dataset with {collected_so_far} tracks.\")\n",
    "\n",
    "    if \"id\" in existing_df.columns:\n",
    "        existing_ids = set(existing_df[\"id\"])\n",
    "    else:\n",
    "        print(\"Warning: 'id' column not found. Deduplication will fallback.\")\n",
    "        existing_ids = set()\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    collected_so_far = 0\n",
    "    existing_ids = set()\n",
    "\n",
    "def extract_year(date_str):\n",
    "    try:\n",
    "        year = int(str(date_str)[:4])\n",
    "        if YEAR_MIN <= year <= YEAR_MAX:\n",
    "            return year\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "#  Fetch track with retry for quota-hit\n",
    "def fetch_track(track_id, retries=3):\n",
    "    if pd.isna(track_id):\n",
    "        print(f\"Skipping invalid track ID: {track_id}\")\n",
    "        return None\n",
    "\n",
    "    url = f\"https://api.deezer.com/track/{track_id}\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if \"error\" in data:\n",
    "                    if \"Quota limit exceeded\" in data[\"error\"].get(\"message\", \"\"):\n",
    "                        retry_after = response.headers.get(\"Retry-After\")\n",
    "                        if retry_after:\n",
    "                            # Use the retry after value in seconds\n",
    "                            wait_time = int(retry_after)\n",
    "                            print(f\"Quota hit, waiting {wait_time} seconds to retry\")\n",
    "                            time.sleep(wait_time)\n",
    "                        else:\n",
    "                            # Fallback strategy \n",
    "                            wait_time = 60 * (2 ** attempt)  # Exponential backoff\n",
    "                            print(f\"Quota hit, waiting {wait_time} seconds to retry (attempt {attempt+1})\")\n",
    "                            time.sleep(wait_time)\n",
    "                        return \"quota_hit\"\n",
    "                    print(f\"Track {track_id} skipped due to API error.\")\n",
    "                    return None\n",
    "                return {\n",
    "                    \"id\": data.get(\"id\"),\n",
    "                    \"title\": data.get(\"title\"),\n",
    "                    \"artist\": data[\"artist\"][\"name\"] if \"artist\" in data else \"Unknown\",\n",
    "                    \"album\": data[\"album\"][\"title\"] if \"album\" in data else \"Unknown\",\n",
    "                    \"duration\": data.get(\"duration\", 0),\n",
    "                    \"rank\": data.get(\"rank\", 0),\n",
    "                    \"explicit_lyrics\": data.get(\"explicit_lyrics\", False),\n",
    "                    \"release_date\": data.get(\"release_date\", \"Unknown\"),\n",
    "                    \"isrc\": data.get(\"isrc\", \"Unknown\"),\n",
    "                    \"deezer_link\": f\"https://www.deezer.com/track/{data['id']}\"\n",
    "                }\n",
    "\n",
    "            elif response.status_code == 403:\n",
    "                return \"quota_hit\"\n",
    "\n",
    "            else:\n",
    "                print(f\"Error fetching track {track_id}: HTTP {response.status_code}\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching track {track_id}: {e}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "    return None\n",
    "\n",
    "#  Cleaning logic with retry \n",
    "def clean_existing_rows(df):\n",
    "    required_fields = ['title', 'release_date', 'isrc']\n",
    "    missing_rows = df[df[required_fields].isnull().any(axis=1)]\n",
    "    print(f\"Found {len(missing_rows)} tracks with missing values to clean.\")\n",
    "    for idx, row in missing_rows.iterrows():\n",
    "        missing_fields = [field for field in required_fields if pd.isna(row[field])]\n",
    "        print(f\"Track ID {row['id']} is missing: {', '.join(missing_fields)}\")\n",
    "\n",
    "    cleaned_batches = []\n",
    "    cleaned_count = 0\n",
    "    skipped_count = 0\n",
    "    dropped_count = 0\n",
    "    quota_retry_list = []\n",
    "    saved_count = 0\n",
    "    skipped_tracks_log = []\n",
    "\n",
    "    def process_rows(rows_to_clean):\n",
    "        nonlocal cleaned_batches, cleaned_count, skipped_count, saved_count, quota_retry_list\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            future_to_row = {executor.submit(fetch_track, row['id']): row for _, row in rows_to_clean.iterrows()}\n",
    "            for future in tqdm(concurrent.futures.as_completed(future_to_row), total=len(future_to_row), desc=\"Cleaning tracks\", unit=\"track\"):\n",
    "                row = future_to_row[future]\n",
    "                result = future.result()\n",
    "\n",
    "                if result == \"quota_hit\":\n",
    "                    quota_retry_list.append(row)\n",
    "                    continue\n",
    "\n",
    "                if result:\n",
    "                    updated_row = row.copy()\n",
    "                    for key, value in result.items():\n",
    "                        if pd.isna(updated_row.get(key)) and value is not None:\n",
    "                            updated_row[key] = value\n",
    "                    cleaned_batches.append(updated_row)\n",
    "                    cleaned_count += 1\n",
    "                    saved_count += 1\n",
    "\n",
    "                    if saved_count % TRACKS_TO_SAVE_EVERY == 0:\n",
    "                        cleaned_df = pd.DataFrame(cleaned_batches)\n",
    "                        existing_df = pd.concat([df[~df.index.isin(missing_rows.index)], cleaned_df], ignore_index=True)\n",
    "                        existing_df.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "                        existing_df.to_csv(CSV_FILE, index=False)\n",
    "                        print(f\"Saved cleaned dataset with {len(existing_df)} total unique tracks (saved {saved_count} tracks so far)\")\n",
    "                        cleaned_batches = []\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "                    skipped_tracks_log.append(f\"Track {row['id']} skipped\")\n",
    "\n",
    "    process_rows(missing_rows)\n",
    "\n",
    "    # Retry if quota was hit \n",
    "    if quota_retry_list:\n",
    "        print(f\"Quota hit, waiting 60 minutes to retry {len(quota_retry_list)} tracks\")\n",
    "        time.sleep(3600)\n",
    "        quota_df = pd.DataFrame(quota_retry_list)\n",
    "        quota_retry_list.clear()\n",
    "        process_rows(quota_df)\n",
    "\n",
    "    cleaned_df = pd.DataFrame(cleaned_batches)\n",
    "    combined_df = pd.concat([df[~df.index.isin(missing_rows.index)], cleaned_df], ignore_index=True)\n",
    "\n",
    "    required_columns = ['id', 'title', 'release_date']\n",
    "    before_drop = len(combined_df)\n",
    "    combined_df.dropna(subset=required_columns, inplace=True)\n",
    "    after_drop = len(combined_df)\n",
    "    dropped_count = before_drop - after_drop\n",
    "\n",
    "    print(f\"Cleaning summary:\")\n",
    "    print(f\"Cleaned and updated: {cleaned_count}\")\n",
    "    print(f\"Skipped (not found or invalid): {skipped_count}\")\n",
    "    print(f\"Dropped rows with missing critical data: {dropped_count}\")\n",
    "\n",
    "    if skipped_tracks_log:\n",
    "        with open(\"skipped_tracks_log.txt\", \"w\") as log_file:\n",
    "            for log in skipped_tracks_log:\n",
    "                log_file.write(log + \"\\n\")\n",
    "        print(\"Skipped track details saved to 'skipped_tracks_log.txt'\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "if collected_so_far > 0:\n",
    "    if not os.path.exists(CLEANING_DONE_FILE):\n",
    "        print(\"Cleaning tracks\")\n",
    "        cleaned_df = clean_existing_rows(existing_df)\n",
    "\n",
    "        if not cleaned_df.empty:\n",
    "            existing_df = cleaned_df\n",
    "            existing_df.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "            existing_df.to_csv(CSV_FILE, index=False)\n",
    "            print(f\"Saved cleaned dataset with {len(existing_df)} total unique tracks\")\n",
    "\n",
    "            with open(CLEANING_DONE_FILE, \"w\") as done_file:\n",
    "                done_file.write(\"done\")\n",
    "        else:\n",
    "            print(\"No cleaned data to save.\")\n",
    "    else:\n",
    "        print(\"Skipping cleaning simce already done.\")\n",
    "else:\n",
    "    print(\"No existing data to clean.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3fad5f-ac66-42b2-addf-cd76284e0759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Reset cleaning \n",
    "if os.path.exists(CLEANING_DONE_FILE):\n",
    "    os.remove(CLEANING_DONE_FILE)\n",
    "    print(\"cleaning_done.txt deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742d005-b56b-4f3c-9155-82f682cbd536",
   "metadata": {},
   "source": [
    "## Final polish for cleaned dataset, kept the original deezer_tracks_super_dataset1 and then made a copy and  renamed it to deezer_tracks_2000_2025.csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef32c2-7025-42ca-84a0-b812a114e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Constants for directory\n",
    "PROCESSED_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../Data/Processed\"))\n",
    "\n",
    "# File paths\n",
    "csv_file = os.path.join(PROCESSED_DIR, \"deezer_tracks_2000_2025.csv\")\n",
    "final_csv = os.path.join(PROCESSED_DIR, \"deezer_tracks_2000_2025.csv\")\n",
    "\n",
    "df = pd.read_csv(csv_file, low_memory=False)\n",
    "\n",
    "# Drop 'source' column \n",
    "if \"source\" in df.columns:\n",
    "    df.drop(columns=[\"source\"], inplace=True)\n",
    "\n",
    "# Convert 'id' to Int64 (nullable int type)\n",
    "df[\"id\"] = df[\"id\"].astype(\"Int64\")\n",
    "\n",
    "df[\"deezer_link\"] = df[\"id\"].apply(lambda x: f\"https://www.deezer.com/track/{x}\" if pd.notna(x) else \"\")\n",
    "\n",
    "desired_order = [\n",
    "    \"title\", \"artist\", \"album\", \"duration\", \"deezer_link\", \"rank\",\n",
    "    \"explicit_lyrics\", \"release_date\", \"id\", \"isrc\"\n",
    "]\n",
    "\n",
    "final_columns = [col for col in desired_order if col in df.columns]\n",
    "df = df[final_columns]\n",
    "\n",
    "# Save to file\n",
    "df.to_csv(final_csv, index=False)\n",
    "print(f\"Polished dataset saved as {final_csv} with {len(df)} tracks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2666a4-c5d4-413a-be06-4eb2fad12e80",
   "metadata": {},
   "source": [
    "## Filtering by year again to be sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91a4b0-61ad-48f7-b907-5e6f32c5e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Constants for directory\n",
    "PROCESSED_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../Data/Processed\"))\n",
    "\n",
    "csv_file = os.path.join(PROCESSED_DIR, \"deezer_tracks_2000_2025.csv\")\n",
    "final_csv = os.path.join(PROCESSED_DIR, \"deezer_tracks_2000_2025.csv\")\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file, low_memory=False)\n",
    "\n",
    "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")\n",
    "\n",
    "# Extract release year\n",
    "df[\"release_year\"] = df[\"release_date\"].dt.year\n",
    "\n",
    "# Filter for years between 2000 and 2025 (inclusive)\n",
    "df_filtered = df[(df[\"release_year\"] >= 2000) & (df[\"release_year\"] <= 2025)].copy()\n",
    "\n",
    "df_filtered.drop(columns=[\"release_year\"], inplace=True)\n",
    "\n",
    "# Save filtered dataset\n",
    "df_filtered.to_csv(final_csv, index=False)\n",
    "\n",
    "print(f\"dataset saved as {final_csv} with {len(df_filtered)} tracks from 2000 to 2025.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
